{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn S3\n",
    "\n",
    "### In this short notebook, we will go through how to perform a number of common tasks associated with S3 buckets. These tasks include:\n",
    "\n",
    "- **Creating a bucket**\n",
    "- **Adding objects to the bucket**\n",
    "- **Downloading an object from a bucket**\n",
    "- **Modifying an object or a bucket**\n",
    "- **Deleting objects from a bucket**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Add a blank line for vertical space -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Necessary Libraries\n",
    "\n",
    "### Only one library is required to work with S3 buckets, but we will install a few more. Here are the libraries we will install:\n",
    "\n",
    "- **boto3**  \n",
    "  This library is how you will be interacting with the S3 buckets and other Amazon services.\n",
    "- **dotenv**  \n",
    "  This library will allow you to store your credentials separately from the code.\n",
    "- **faker**  \n",
    "  This library will allow us to generate some dummy data to store in your S3 bucket.\n",
    "\n",
    "### Setting Up the Virtual Environment\n",
    "\n",
    "Before installing these libraries, ensure you are utilizing a virtual environment. You can create one by using this command in your terminal:\n",
    "`python -m venv s3_practice_venv`\n",
    "\n",
    "### You then activate the virtual environment by typing this in your terminal\n",
    "`.\\s3_practice_venv\\Scripts\\Activate.ps1`\n",
    "\n",
    "\n",
    "\n",
    "### Once the virtual environment is activated, you can install the necessary libraries\n",
    "`python -m pip install python-dotenv faker boto3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an amazon free tier account\n",
    "\n",
    "### In order to complete these exercises, an amazon free (or paid) account is required. To create this account, navigate to this link\n",
    "[Amazon Free Tier Account](https://aws.amazon.com/free/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to your Amazon account\n",
    "\n",
    "Before we go into creating buckets, we need to connect to our Amazon account. We can do this pretty easily using boto3 (the python library for Amazon services).\n",
    "\n",
    "There are multiple ways to do this. For the sake of this notebook, we will just stick to one method. There are two things we need before we connect, the aws access key and the aws secret access key. You can generate this by navigating to your profile in the top right and selecting \"Security Credentials\"\n",
    "\n",
    "![Image displaying profile options](Profile.png)\n",
    "\n",
    "Once you select that, scroll down until you see the section titled \"Access keys\". You can then select the option \"Create access key\"\n",
    "\n",
    "![Image displaying the access key screen](Access.png)\n",
    "\n",
    "After creating your access keys, we need to put them somewhere separated from our code. We will use the dotenv library to help us with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the .env file\n",
    "In the same folder as your python file, create a file called \".env\" (with no quotes). Then you can place your access keys inside of this file in key value pairs (the keys don't matter). Below is an example,\n",
    "\n",
    "![Example of how to structure the .env file](env_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the keys into our file\n",
    "\n",
    "To load the keys into our file, we need to use the \"dotenv\" library and also the \"os\" library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "access_key = os.getenv(\"ACCESS\")\n",
    "secret_access_key = os.getenv(\"SECRET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally we can connect to our amazon account using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = access_key,\n",
    "    aws_secret_access_key = secret_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have connected, we can start utilizing S3.\n",
    "\n",
    "### First thing we need to do is specify that we are utilizing S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing session object we created earlier to create an S3 resource object\n",
    "S3 = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can utilize Amazon S3 within our python file.\n",
    "\n",
    "I won't go into every functionality that boto3 has for Amazon S3. If you would like to learn more, you can read the documentation [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an S3 Bucket\n",
    "\n",
    "We can easily create a bucket utilizing the .create_bucket method. \n",
    "The .create_bucket method can take in a location but for our case we will just stick to providing a name only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 object we created above\n",
    "S3.create_bucket(Bucket = \"Bucket_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above most likely will not work. The reason for this is due to Amazon's strict bucket naming requirements.\n",
    "Due to buckets being in a global namespace, your name must be unique. On top of the uniqueness requirements,\n",
    "you will also need to follow the other requirements that are detailed [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html?icmpid=docs_amazons3_console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we have created our bucket, we can connect to it and send and receive information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again utilizing the S3 object we created before, but this time we are using it to grab our bucket object\n",
    "bucket = S3.Bucket(\"Bucket_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we try sending files to our bucket, let's make a few.\n",
    "Run the below code to generate two files, fake_data.csv and example_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "fake_csv = fake.csv(\n",
    "    [\"FirstName\", \"LastName\", \"PhoneNumber\", \"Birthday\"],\n",
    "    data_columns = (\"{{first_name}}\", \"{{last_name}}\", \"{{basic_phone_number}}\", \"{{date_of_birth}}\"),\n",
    "    num_rows = 500\n",
    ")\n",
    "fake_csv = \"\".join(fake_csv.split(\"\\n\"))\n",
    "\n",
    "with open('fake_data.csv', 'w') as f:\n",
    "    f.write(fake_csv)\n",
    "\n",
    "with open(\"example_text.txt\", \"w\") as f:\n",
    "    f.write(\"Updog\\nWhat's updog?\\nNothing much dog, what's up with you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can send our files to our bucket using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .upload_file method takes in the name of the local file and also the name you are giving it in your S3 bucket. They do not need to be the same.\n",
    "for file in [\"fake_data.csv\", \"example_text.txt\"]:\n",
    "    bucket.upload_file(file, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's download one of the files using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .download_file method takes in the name of the file in the S3 bucket and then the name you are assigning it locally\n",
    "bucket.download_file(\"fake_data.csv\", \"real_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's clear out our S3 bucket (THIS IS NOT SOMETHING YOU WOULD DO NORMALLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for object in bucket.objects.all():\n",
    "    object.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Checks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Create a new bucket and insert a file\n",
    "\n",
    "1. Create a new bucket below, it can be named whatever you would like as long as it is unique and follows Amazon's bucket naming requirements.\n",
    "2. Insert the file provided into the bucket\n",
    "3. Return the file name of the file in the bucket\n",
    "\n",
    "NOTE: Use bucket as your variable name to connect to the bucket so that the grading function can check your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_file import create_file\n",
    "\n",
    "# All code should go inside this function, do not rename or delete\n",
    "def create_and_upload():\n",
    "    file_name = create_file() # Use this file to send to your bucket\n",
    "    # Code goes here\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your work here by running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert bucket.Object(create_and_upload()).get()[\"Body\"].read().decode('utf-8') == \"Hello world!\", \"The text in your file does not match the text provided.\"\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Iterate through all buckets (Should be two at this point) and print the name of every object within each one\n",
    "\n",
    "1. Iterate through all buckets\n",
    "2. Iterate through every object in each bucket\n",
    "3. Print the name of each file\n",
    "\n",
    "HINT: This involves some things I did not teach you. Try to figure it out without google/chatgpt. If you get stuck, use this [link](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/collections.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All code should go inside this function\n",
    "def create_and_upload():\n",
    "    # Code goes here\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
